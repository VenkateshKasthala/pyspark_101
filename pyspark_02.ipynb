{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/08 16:46:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/08 16:46:48 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Filtering').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('/Users/kvenkateshrao/Downloads/Tableau Full course/Season.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------+-------------+--------------------+\n",
      "|Season_Id|Season_Year|Orange_Cap_Id|Purple_Cap_Id|Man_of_the_Series_Id|\n",
      "+---------+-----------+-------------+-------------+--------------------+\n",
      "|        1|       2008|          100|          102|                  32|\n",
      "|        2|       2009|           18|           61|                  53|\n",
      "|        3|       2010|          133|          131|                 133|\n",
      "|        4|       2011|          162|          194|                 162|\n",
      "|        5|       2012|          162|          190|                 315|\n",
      "|        6|       2013|           19|           71|                  32|\n",
      "|        7|       2014|           46|          364|                 305|\n",
      "|        8|       2015|          187|           71|                 334|\n",
      "|        9|       2016|            8|          299|                   8|\n",
      "+---------+-----------+-------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single column selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Season_Id|\n",
      "+---------+\n",
      "|        1|\n",
      "|        2|\n",
      "|        3|\n",
      "|        4|\n",
      "|        5|\n",
      "|        6|\n",
      "|        7|\n",
      "|        8|\n",
      "|        9|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Season_Id').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple column selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|Season_Id|Season_Year|\n",
      "+---------+-----------+\n",
      "|        1|       2008|\n",
      "|        2|       2009|\n",
      "|        3|       2010|\n",
      "|        4|       2011|\n",
      "|        5|       2012|\n",
      "|        6|       2013|\n",
      "|        7|       2014|\n",
      "|        8|       2015|\n",
      "|        9|       2016|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Season_Id','Season_Year']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------+-------------+--------------------+\n",
      "|Season_Id|Season_Year|Orange_Cap_Id|Purple_Cap_Id|Man_of_the_Series_Id|\n",
      "+---------+-----------+-------------+-------------+--------------------+\n",
      "|        4|       2011|          162|          194|                 162|\n",
      "|        5|       2012|          162|          190|                 315|\n",
      "|        6|       2013|           19|           71|                  32|\n",
      "|        7|       2014|           46|          364|                 305|\n",
      "|        8|       2015|          187|           71|                 334|\n",
      "|        9|       2016|            8|          299|                   8|\n",
      "+---------+-----------+-------------+-------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/01/08 16:47:05 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(df_pyspark.Season_Year>2010).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can filter using comparison operators (>, <, >=, etc.) and string functions like startswith(), contains(), and endswith()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------------+-------------+--------------------+\n",
      "|Season_Id|Season_Year|Orange_Cap_Id|Purple_Cap_Id|Man_of_the_Series_Id|\n",
      "+---------+-----------+-------------+-------------+--------------------+\n",
      "|        4|       2011|          162|          194|                 162|\n",
      "|        5|       2012|          162|          190|                 315|\n",
      "|        6|       2013|           19|           71|                  32|\n",
      "|        7|       2014|           46|          364|                 305|\n",
      "+---------+-----------+-------------+-------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark.Season_Year>2010)&(df_pyspark.Season_Id<=7)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming and Dropping Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-------------+-------------+\n",
      "|Season_Id|Year|Orange_Cap_Id|Purple_Cap_Id|\n",
      "+---------+----+-------------+-------------+\n",
      "|        1|2008|          100|          102|\n",
      "|        2|2009|           18|           61|\n",
      "|        3|2010|          133|          131|\n",
      "|        4|2011|          162|          194|\n",
      "|        5|2012|          162|          190|\n",
      "|        6|2013|           19|           71|\n",
      "|        7|2014|           46|          364|\n",
      "|        8|2015|          187|           71|\n",
      "|        9|2016|            8|          299|\n",
      "+---------+----+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark=df_pyspark.withColumnRenamed('Season_Year','Year')\n",
    "df_pyspark=df_pyspark.drop('Man_of_the_Series_Id')\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a New Column With a Constant Value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-------------+-------------+-------+\n",
      "|Season_Id|Year|Orange_Cap_Id|Purple_Cap_Id|Country|\n",
      "+---------+----+-------------+-------------+-------+\n",
      "|        1|2008|          100|          102|  India|\n",
      "|        2|2009|           18|           61|  India|\n",
      "|        3|2010|          133|          131|  India|\n",
      "|        4|2011|          162|          194|  India|\n",
      "|        5|2012|          162|          190|  India|\n",
      "|        6|2013|           19|           71|  India|\n",
      "|        7|2014|           46|          364|  India|\n",
      "|        8|2015|          187|           71|  India|\n",
      "|        9|2016|            8|          299|  India|\n",
      "+---------+----+-------------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df_pyspark = df_pyspark.withColumn(\"Country\", lit(\"India\")) \n",
    "df_pyspark.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Expressions and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+-------------+-------------+-------+------------------+\n",
      "|Season_Id|Year|Orange_Cap_Id|Purple_Cap_Id|Country|Year after 2 years|\n",
      "+---------+----+-------------+-------------+-------+------------------+\n",
      "|        1|2008|          100|          102|  India|              2010|\n",
      "|        2|2009|           18|           61|  India|              2011|\n",
      "|        3|2010|          133|          131|  India|              2012|\n",
      "|        4|2011|          162|          194|  India|              2013|\n",
      "|        5|2012|          162|          190|  India|              2014|\n",
      "|        6|2013|           19|           71|  India|              2015|\n",
      "|        7|2014|           46|          364|  India|              2016|\n",
      "|        8|2015|          187|           71|  India|              2017|\n",
      "|        9|2016|            8|          299|  India|              2018|\n",
      "+---------+----+-------------+-------------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark=df_pyspark.withColumn('Year after 2 years', df_pyspark.Year+2)\n",
    "df_pyspark.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
